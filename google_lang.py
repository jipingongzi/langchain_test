from pathlib import Path
from dotenv import load_dotenv
import os
import warnings
import shutil
from PIL import Image
import pytesseract
from easyocr import Reader
from transformers import BlipProcessor, BlipForConditionalGeneration
from chromadb import PersistentClient
from chromadb.utils import embedding_functions

warnings.filterwarnings("ignore", category=FutureWarning)
warnings.filterwarnings("ignore", category=DeprecationWarning)

from langchain.document_loaders import UnstructuredPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain.embeddings.base import Embeddings
from langchain.chains import RetrievalQA
from langchain_openai import ChatOpenAI
from langchain.schema import Document
from langchain.prompts import PromptTemplate  # æ–°å¢å¯¼å…¥
from typing import List, Tuple


class ImageProcessor:
    def __init__(self):
        self.tesseract_lang = "eng+chi_sim"
        self.easyocr_reader = Reader(['en', 'ch_sim'], gpu=False)
        self.blip_processor = BlipProcessor.from_pretrained("Salesforce/blip-image-captioning-base")
        self.blip_model = BlipForConditionalGeneration.from_pretrained("Salesforce/blip-image-captioning-base")
        print("âœ… å›¾ç‰‡å¤„ç†å™¨åˆå§‹åŒ–å®Œæˆï¼ˆOCR+è¯­ä¹‰æè¿°ï¼‰")

    def ocr_image(self, img_path: Path) -> str:
        try:
            img = Image.open(img_path)
            ocr_text = pytesseract.image_to_string(img, lang=self.tesseract_lang)
            print("-------------------:" + ocr_text)
            if ocr_text.strip():
                return f"ã€å›¾ç‰‡OCRæ–‡å­—ã€‘ï¼š{ocr_text.strip()}"
            else:
                result = self.easyocr_reader.readtext(str(img_path), detail=0)
                easyocr_text = "\n".join(result)
                return f"ã€å›¾ç‰‡OCRæ–‡å­—ã€‘ï¼š{easyocr_text.strip() if easyocr_text else 'æœªè¯†åˆ«åˆ°æ–‡å­—'}"
        except Exception as e:
            return f"ã€å›¾ç‰‡OCRé”™è¯¯ã€‘ï¼š{str(e)[:50]}"

    def generate_image_caption(self, img_path: Path) -> str:
        try:
            img = Image.open(img_path).convert("RGB")
            inputs = self.blip_processor(img, return_tensors="pt")
            out = self.blip_model.generate(**inputs, max_length=50)
            caption = self.blip_processor.decode(out[0], skip_special_tokens=True)
            return f"ã€å›¾ç‰‡è¯­ä¹‰æè¿°ã€‘ï¼š{caption}"
        except Exception as e:
            return f"ã€å›¾ç‰‡æè¿°é”™è¯¯ã€‘ï¼š{str(e)[:50]}"

    def process_single_image(self, img_path: Path) -> str:
        if not img_path.exists():
            return "ã€å›¾ç‰‡ä¸å­˜åœ¨ã€‘"
        ocr_text = self.ocr_image(img_path)
        caption_text = self.generate_image_caption(img_path)
        return f"\n=== å›¾ç‰‡ä¿¡æ¯ï¼ˆè·¯å¾„ï¼š{img_path.name}ï¼‰===\n{caption_text}\n{ocr_text}\n"


def load_pdf_with_images(pdf_path: str, temp_img_dir: str = "./temp_pdf_images") -> List[Document]:
    pdf_path = Path(pdf_path)
    temp_img_dir = Path(temp_img_dir)

    if not pdf_path.exists():
        raise FileNotFoundError(f"PDFæ–‡ä»¶ä¸å­˜åœ¨ï¼š{pdf_path.resolve()}")

    if temp_img_dir.exists():
        shutil.rmtree(temp_img_dir)
    temp_img_dir.mkdir(exist_ok=True)
    print(f"ğŸ“ ä¸´æ—¶å›¾ç‰‡ç›®å½•ï¼š{temp_img_dir.resolve()}")

    loader = UnstructuredPDFLoader(
        str(pdf_path),
        strategy="fast",
        extract_images_in_pdf=True,
        image_output_dir_path=str(temp_img_dir)
    )
    docs = loader.load()
    print(f"âœ… åŠ è½½PDFå®Œæˆï¼ˆå…±{len(docs)}é¡µï¼‰ï¼Œæå–å›¾ç‰‡æ•°é‡ï¼š{len(list(temp_img_dir.glob('*.png')))}")

    img_processor = ImageProcessor()
    merged_docs = []
    for page_idx, doc in enumerate(docs, 1):
        page_text = doc.page_content.strip() or "ã€æœ¬é¡µæ— æ–‡å­—ã€‘"

        page_images = list(temp_img_dir.glob(f"page_{page_idx}_image_*.png"))
        if page_images:
            image_texts = [img_processor.process_single_image(img) for img in page_images]
            merged_text = f"=== PDFç¬¬{page_idx}é¡µ ===\n{page_text}\n" + "\n".join(image_texts)
        else:
            merged_text = f"=== PDFç¬¬{page_idx}é¡µ ===\n{page_text}"

        merged_doc = Document(
            page_content=merged_text,
            metadata={"page": page_idx, "source": str(pdf_path.name)}
        )
        merged_docs.append(merged_doc)

    # shutil.rmtree(temp_img_dir)
    print(f"âœ… PDFå›¾æ–‡åˆå¹¶å®Œæˆï¼ˆå…±{len(merged_docs)}é¡µï¼Œå›¾ç‰‡å·²è½¬ä¸ºæ–‡æœ¬ï¼‰")
    return merged_docs


class UniversalEmbeddingsAdapter(Embeddings):
    def __init__(self, chroma_embedding_fn):
        self.chroma_embedding_fn = chroma_embedding_fn

    def embed_documents(self, texts: List[str]) -> List[List[float]]:
        try:
            return self.chroma_embedding_fn.embed_texts(texts)
        except AttributeError:
            return self.chroma_embedding_fn(texts)

    def embed_query(self, text: str) -> List[float]:
        try:
            return self.chroma_embedding_fn.embed_texts([text])[0]
        except AttributeError:
            return self.chroma_embedding_fn([text])[0]


def init_deepseek_llm() -> ChatOpenAI:
    deepseek_api_key = os.getenv("DEEPSEEK_API_KEY", "sk-9b5776bd68e045f7ae2171077134b2a4")
    if not deepseek_api_key.startswith("sk-"):
        raise ValueError("DeepSeek APIå¯†é’¥æ ¼å¼é”™è¯¯ï¼Œåº”ä»¥'sk-'å¼€å¤´")

    return ChatOpenAI(
        api_key=deepseek_api_key,
        base_url="https://api.deepseek.com/v1",
        model="deepseek-chat",
        temperature=0.3,
        timeout=60,
        max_retries=2
    )


def build_vector_store(merged_docs: List[Document]) -> Chroma:
    chroma_client = PersistentClient(path="./chroma_pdf_with_images_db")
    collection_name = "pdf_qa_with_images"

    embedding_fn = embedding_functions.SentenceTransformerEmbeddingFunction(
        model_name="paraphrase-multilingual-MiniLM-L12-v2"
    )
    langchain_embeddings = UniversalEmbeddingsAdapter(embedding_fn)

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
    split_docs = text_splitter.split_documents(merged_docs)
    print(f"âœ‚ï¸  æ–‡æœ¬åˆ†å‰²å®Œæˆï¼ˆå«å›¾ç‰‡è½¬æ–‡æœ¬ï¼‰ï¼Œå…±{len(split_docs)}ä¸ªæ–‡æœ¬å—")

    collections = chroma_client.list_collections()
    collection_exists = any(col.name == collection_name for col in collections)

    if collection_exists:
        print(f"ğŸ“¦ åŠ è½½å·²å­˜åœ¨çš„å‘é‡åº“")
        vector_store = Chroma(
            client=chroma_client,
            collection_name=collection_name,
            embedding_function=langchain_embeddings
        )
        collection = chroma_client.get_collection(name=collection_name)
        if collection.count() == 0:
            vector_store.add_documents(split_docs)
            print(f"âœ… å·²æ·»åŠ {len(split_docs)}ä¸ªæ–‡æœ¬å—åˆ°å‘é‡åº“")
    else:
        print(f"ğŸš€ åˆ›å»ºæ–°å‘é‡åº“ï¼ˆå«å›¾æ–‡ä¿¡æ¯ï¼‰")
        vector_store = Chroma.from_documents(
            documents=split_docs,
            embedding=langchain_embeddings,
            client=chroma_client,
            collection_name=collection_name
        )

    return vector_store


def build_qa_chain(vector_store: Chroma, llm: ChatOpenAI) -> RetrievalQA:
    retriever = vector_store.as_retriever(
        search_kwargs={"k": 4}
    )

    # å…³é”®ä¿®å¤ï¼šå°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºPromptTemplateå®ä¾‹
    prompt_template = """
    åŸºäºä»¥ä¸‹å‚è€ƒå†…å®¹ï¼ˆåŒ…å«PDFçš„æ–‡å­—å’Œå›¾ç‰‡è½¬æ–‡æœ¬ä¿¡æ¯ï¼‰å›ç­”ç”¨æˆ·é—®é¢˜ï¼š
    {context}

    å›ç­”è¦æ±‚ï¼š
    1. è‹¥å‚è€ƒå†…å®¹ä¸­æœ‰â€œå›¾ç‰‡OCRæ–‡å­—â€æˆ–â€œå›¾ç‰‡è¯­ä¹‰æè¿°â€ï¼Œå¿…é¡»ä¼˜å…ˆç»“åˆè¿™äº›ä¿¡æ¯å›ç­”ï¼›
    2. æ˜ç¡®åŒºåˆ†æ–‡å­—ä¿¡æ¯å’Œå›¾ç‰‡ä¿¡æ¯ï¼ˆå¦‚â€œæ ¹æ®PDFç¬¬3é¡µçš„å›¾ç‰‡æè¿°...â€ï¼‰ï¼›
    3. è‹¥å‚è€ƒå†…å®¹æ— ç›¸å…³ä¿¡æ¯ï¼Œç›´æ¥å›å¤â€œæœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯â€ï¼›
    4. ç”¨è‡ªç„¶è¯­è¨€ï¼Œä¿æŒç®€æ´å‡†ç¡®ã€‚

    ç”¨æˆ·é—®é¢˜ï¼š{question}
    å›ç­”ï¼š
    """
    # è½¬æ¢ä¸ºPromptTemplateæ ¼å¼
    prompt = PromptTemplate(
        template=prompt_template,
        input_variables=["context", "question"]
    )

    return RetrievalQA.from_chain_type(
        llm=llm,
        chain_type="stuff",
        retriever=retriever,
        return_source_documents=True,
        chain_type_kwargs={"prompt": prompt}  # ä½¿ç”¨æ­£ç¡®çš„æ¨¡æ¿å¯¹è±¡
    )


def interactive_qa(qa_chain: RetrievalQA):
    print("\n=== PDFå›¾æ–‡çŸ¥è¯†é—®ç­”ç³»ç»Ÿ ===")
    print("æç¤ºï¼šè¾“å…¥ 'exit' æˆ– 'é€€å‡º' ç»“æŸå¯¹è¯ï¼ˆæ”¯æŒé—®å›¾ç‰‡ç›¸å…³é—®é¢˜ï¼‰")
    while True:
        user_question = input("\nè¯·è¾“å…¥ä½ çš„é—®é¢˜ï¼š")
        if user_question.lower() in ["exit", "é€€å‡º"]:
            print("å¯¹è¯ç»“æŸï¼Œå†è§ï¼")
            break
        if not user_question.strip():
            print("è¯·è¾“å…¥æœ‰æ•ˆé—®é¢˜")
            continue

        try:
            result = qa_chain.invoke({"query": user_question})
            print("\n=== å›ç­” ===")
            print(result["result"])

            print("\n=== å‚è€ƒæ¥æºï¼ˆå«å›¾ç‰‡è½¬æ–‡æœ¬ä¿¡æ¯ï¼‰ ===")
            for i, doc in enumerate(result["source_documents"], 1):
                page_num = doc.metadata.get("page", "æœªçŸ¥")
                source_preview = doc.page_content[:200].replace("\n", " ") + "..."
                print(f"{i}. PDFç¬¬{page_num}é¡µï¼š{source_preview}")
        except Exception as e:
            error_msg = str(e).lower()
            if "timed out" in error_msg:
                print("\nâŒ è¯·æ±‚è¶…æ—¶ï¼šæ£€æŸ¥ç½‘ç»œæˆ–DeepSeek APIçŠ¶æ€")
            else:
                print(f"\nâŒ é”™è¯¯ï¼š{str(e)}")


if __name__ == "__main__":
    load_dotenv()
    pdf_path = "TDD EPAM BMS system.pdf"

    try:
        print("=== åˆå§‹åŒ–PDFå›¾æ–‡é—®ç­”ç³»ç»Ÿ ===")
        llm = init_deepseek_llm()
        print("âœ… DeepSeek LLMåˆå§‹åŒ–å®Œæˆ")

        merged_docs = load_pdf_with_images(pdf_path)

        vector_store = build_vector_store(merged_docs)
        print("âœ… å‘é‡åº“æ„å»ºå®Œæˆï¼ˆå«å›¾ç‰‡è½¬æ–‡æœ¬ä¿¡æ¯ï¼‰")

        qa_chain = build_qa_chain(vector_store, llm)
        interactive_qa(qa_chain)

    except Exception as e:
        print(f"\nâŒ åˆå§‹åŒ–å¤±è´¥ï¼š{str(e)}")
