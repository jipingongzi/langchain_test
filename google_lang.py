from langchain_unstructured import UnstructuredLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain.embeddings import HuggingFaceEmbeddings
from chromadb import PersistentClient
from langchain_openai import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate


def merge_related_docs(raw_docs):
    """åˆå¹¶åŸå§‹æ–‡æ¡£ä¸­åŒé¡µç ã€åŒç±»å‹çš„å…³è”å…ƒç´ ï¼ˆå¦‚æ ‡é¢˜+æ­£æ–‡ï¼‰ï¼Œé¿å…æ‹†åˆ†è¿‡ç»†"""
    merged_docs = []
    current_doc = None  # ç´¯ç§¯å½“å‰åˆå¹¶çš„æ–‡æ¡£å¯¹è±¡

    for doc in raw_docs:
        page_num = doc.metadata.get("page_number", "æœªçŸ¥")
        elem_type = doc.metadata.get("element_type", "text")

        if current_doc is None:
            current_doc = doc.copy()
            continue

        current_page = current_doc.metadata.get("page_number", "æœªçŸ¥")
        current_type = current_doc.metadata.get("element_type", "text")

        if (page_num == current_page) and (elem_type == "text") and (current_type == "text"):
            current_doc.page_content += "\n" + doc.page_content
        else:
            merged_docs.append(current_doc)
            current_doc = doc.copy()

    if current_doc is not None:
        merged_docs.append(current_doc)

    return merged_docs


client = PersistentClient(path="./bms_chroma_db")
collection_name = "bms_system_docs"

embeddings = HuggingFaceEmbeddings(
    model_name="BAAI/bge-large-en-v1.5",
    model_kwargs={'device': 'cpu'},
    encode_kwargs={'normalize_embeddings': True}
)

collection = client.get_or_create_collection(name=collection_name)
all_ids = collection.get()["ids"]

if all_ids:
    print(f"âœ… åŠ è½½å·²å­˜åœ¨çš„å‘é‡åº“é›†åˆ: {collection_name}")
    print(f"   é›†åˆåŒ…å« {len(all_ids)} ä¸ªæ–‡æœ¬å—")
    vectorstore = Chroma(
        client=client,
        collection_name=collection_name,
        embedding_function=embeddings
    )
else:
    print(f"ğŸ”§ åˆ›å»ºæ–°çš„å‘é‡åº“é›†åˆ: {collection_name}")
    loader = UnstructuredLoader(
        api_key="iyLmzWtLoVU32XOlivzoBc6aByye8K",
        file_path="./TDD EPAM BMS system.pdf",  # ç¡®ä¿è·¯å¾„æ­£ç¡®
        strategy="hi_res",  # é«˜ç²¾åº¦è§£æï¼ˆä¿ç•™è¡¨æ ¼/å›¾ç‰‡æ ‡è®°ï¼‰
        partition_via_api=True,
        coordinates=True,
    )
    docs = []
    try:
        print("ğŸ“„ æ­£åœ¨åŠ è½½PDFæ–‡æ¡£å†…å®¹...")
        for doc in loader.lazy_load():
            cleaned_metadata = {
                "page_number": doc.metadata.get("page_number", "æœªçŸ¥"),
                "element_type": doc.metadata.get("element_type", "text"),
                "element_id": doc.metadata.get("element_id", "æœªçŸ¥")
            }
            if cleaned_metadata["element_type"] in ["Image", "Table", "Figure"]:
                doc.page_content = f"ã€{cleaned_metadata['element_type']}å†…å®¹ã€‘ï¼š{doc.page_content.strip()}"
            cleaned_doc = doc.copy()
            cleaned_doc.metadata = cleaned_metadata
            docs.append(cleaned_doc)

        print(f"   åŸå§‹æ–‡æ¡£åŠ è½½å®Œæˆï¼Œå…±åŒ…å« {len(docs)} ä¸ªç‹¬ç«‹å…ƒç´ ")
        merged_docs = merge_related_docs(docs)
        print(f"   å…³è”å…ƒç´ åˆå¹¶å®Œæˆï¼Œå…±åŒ…å« {len(merged_docs)} ä¸ªåˆå¹¶åå…ƒç´ ")
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=500,
            chunk_overlap=50,
            length_function=len,
            separators=["\n\n", ". ", "? ", "! ", "; ", "\n", " "]
        )
        split_docs = text_splitter.split_documents(merged_docs)
        print(f"   æ–‡æœ¬åˆ†å‰²å®Œæˆï¼Œå…±ç”Ÿæˆ {len(split_docs)} ä¸ªåˆå§‹æ–‡æœ¬å—")

        min_chunk_length = 20
        filtered_docs = [doc for doc in split_docs if len(doc.page_content) >= min_chunk_length]
        print(
            f"   çŸ­åˆ†å—è¿‡æ»¤å®Œæˆï¼šè¿‡æ»¤ {len(split_docs) - len(filtered_docs)} ä¸ªçŸ­å—ï¼Œå‰©ä½™ {len(filtered_docs)} ä¸ªæœ‰æ•ˆæ–‡æœ¬å—")

        print("\nğŸ“Š å‰3ä¸ªæœ‰æ•ˆåˆ†å—è´¨é‡éªŒè¯ï¼š")
        for i, doc in enumerate(filtered_docs[:3]):
            print(f"\n   åˆ†å—{i + 1}ï¼š")
            print(f"   - é¡µç ï¼šç¬¬{doc.metadata['page_number']}é¡µ")
            print(f"   - é•¿åº¦ï¼š{len(doc.page_content)} å­—ç¬¦")
            print(f"   - å†…å®¹é¢„è§ˆï¼š{doc.page_content[:150]}..." if len(
                doc.page_content) > 150 else f"   - å†…å®¹ï¼š{doc.page_content}")

        vectorstore = Chroma.from_documents(
            documents=filtered_docs,
            embedding=embeddings,
            client=client,
            collection_name=collection_name
        )
        print(f"\nâœ… å‘é‡åº“åˆ›å»ºå®Œæˆï¼Œå·²å†™å…¥ {len(filtered_docs)} ä¸ªæœ‰æ•ˆæ–‡æœ¬å—")

    except Exception as e:
        print(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥ï¼š{str(e)}")
        exit(1)

# -------------------------- 10. åˆå§‹åŒ–LLMï¼ˆDeepSeekï¼‰ --------------------------
llm = ChatOpenAI(
    api_key="sk-9b5776bd68e045f7ae2171077134b2a4",
    base_url="https://api.deepseek.com/v1",
    model="deepseek-chat",
    streaming=False,
    temperature=0.3  # é€‚åº¦æé«˜çµæ´»æ€§ï¼Œé¿å…å›ç­”è¿‡äºåƒµç¡¬
)

# -------------------------- 11. ä¼˜åŒ–æç¤ºè¯æ¨¡æ¿ --------------------------
prompt_template = """ä½ æ˜¯æŠ€æœ¯æ–‡æ¡£é—®ç­”ä¸“å®¶ï¼Œä¸¥æ ¼æŒ‰ä»¥ä¸‹è§„åˆ™å›ç­”ï¼š

1. ä»…ä½¿ç”¨æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œä¸æ·»åŠ ä»»ä½•å¤–éƒ¨çŸ¥è¯†ï¼›
2. ä¼˜å…ˆå¼•ç”¨æ ‡è®°ã€Imageå†…å®¹ã€‘ã€Tableå†…å®¹ã€‘ã€Figureå†…å®¹ã€‘çš„ä¿¡æ¯ï¼Œæ‰€æœ‰è¦ç‚¹å¿…é¡»æ ‡æ³¨æ¥æºé¡µç ï¼ˆå¦‚â€œæ¥æºï¼šç¬¬3é¡µâ€ï¼‰ï¼›
3. è‹¥ä¸Šä¸‹æ–‡åŒ…å«é—®é¢˜ç›¸å…³ä¿¡æ¯ï¼ˆå³ä½¿ä¸å®Œæ•´ï¼‰ï¼Œéœ€åˆ†ç‚¹æ€»ç»“ï¼›è‹¥å®Œå…¨æ— ç›¸å…³ä¿¡æ¯ï¼Œæ‰å›å¤â€œæœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯â€ï¼›
4. ç”¨ç®€æ´çš„è‹±æ–‡å›ç­”ï¼ˆå› ä¸Šä¸‹æ–‡ä¸ºè‹±æ–‡æ–‡æ¡£ï¼‰ï¼Œé¿å…ä¸­è‹±æ–‡æ··æ‚ã€‚

ä¸Šä¸‹æ–‡:
{context}

é—®é¢˜: {question}
å›ç­”:"""

PROMPT = PromptTemplate(
    template=prompt_template,
    input_variables=["context", "question"]
)

# -------------------------- 12. åˆå§‹åŒ–æ£€ç´¢é—®ç­”é“¾ --------------------------
qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    chain_type="stuff",  # é€‚åˆçŸ­ä¸Šä¸‹æ–‡ï¼ˆå·²ä¼˜åŒ–åˆ†å—é•¿åº¦ï¼Œæ— éœ€å…¶ä»–ç±»å‹ï¼‰
    retriever=vectorstore.as_retriever(
        search_kwargs={
            "k": 6
        }
    ),
    chain_type_kwargs={"prompt": PROMPT},
    return_source_documents=True  # è¿”å›æ£€ç´¢åˆ°çš„æºæ–‡æ¡£ï¼Œæ–¹ä¾¿è°ƒè¯•
)

# -------------------------- 13. æµ‹è¯•é—®ç­”æ•ˆæœ --------------------------
test_questions = [
    "what is current problem?",
    "How we fix current problem?",
    "What result can we get?",
    "What is suppliers portal?"
]

print("\n" + "=" * 60)
print("ğŸ“ å¼€å§‹æµ‹è¯•é—®ç­”æ•ˆæœ")
print("=" * 60)

for i, question in enumerate(test_questions, 1):
    print(f"\nã€é—®é¢˜ {i}ã€‘: {question}")
    print("-" * 40)

    try:
        result = qa_chain.invoke({"query": question})

        print("ğŸ” æ£€ç´¢åˆ°çš„ç›¸å…³æ–‡æ¡£ï¼š")
        for j, doc in enumerate(result["source_documents"], 1):
            print(
                f"   æ–‡æ¡£{j}ï¼šç¬¬{doc.metadata['page_number']}é¡µ | {len(doc.page_content)}å­—ç¬¦ | é¢„è§ˆï¼š{doc.page_content[:100]}...")

        print("\nğŸ’¡ LLMå›ç­”ï¼š")
        print(result["result"])

    except Exception as e:
        print(f"âŒ é—®ç­”è¿‡ç¨‹å‡ºé”™ï¼š{str(e)}")

    print("\n" + "-" * 60)